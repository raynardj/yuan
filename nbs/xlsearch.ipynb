{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EtssVJyGkmU"
   },
   "source": [
    "# Cross Language Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using [this nice dataset](https://github.com/BangBOOM/Classical-Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uqq git+https://github.com/raynardj/forgebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgebox.imports import *\n",
    "from forgebox.thunder.callbacks import DataFrameMetricsCallback\n",
    "from forgebox.multiproc import DataFrameRowling\n",
    "from gc_utils.env import *\n",
    "from datasets import load_dataset\n",
    "# from fastai.text.all import *\n",
    "from unpackai.nlp import *\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_all_punkt(text):\n",
    "    \"\"\"\n",
    "    Removes all punctuation from Chinese text.\n",
    "\n",
    "    :param text: text to remove punctuation from\n",
    "    :return: text with no punctuation\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'亳州水军千户胡进等领骑兵渡淝水逾荆山与宋兵战杀获甚众赏钞币有差'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_all_punkt(\"亳州水军千户胡进等领骑兵渡淝水，逾荆山，与宋兵战，杀获甚众，赏钞币有差。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path(sys_loc('DATA')/\"nlp\"/\"zh\"/\"cc_vs_zh\")\n",
    "TO_CLASSICAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbXuwqr0KEr8"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file = list(DATA.rglob(\"data/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file_to_lines(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines\n",
    "\n",
    "def pairing_the_file(files,kw):\n",
    "    pairs = []\n",
    "    for file in files:\n",
    "        if kw not in file.name:\n",
    "            file1 = file\n",
    "            file2 = f\"{file}{kw}\"\n",
    "            pairs.append((file1,file2))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairing_the_file(all_file,\"翻译\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pairs(pairs):\n",
    "    chunks = []\n",
    "    for pair in tqdm(pairs, leave=False):\n",
    "        file1,file2 = pair\n",
    "        lines1 = open_file_to_lines(file1)\n",
    "        lines2 = open_file_to_lines(file2)\n",
    "        chunks.append(pd.DataFrame({\"classical\":lines1,\"modern\":lines2}))\n",
    "    return pd.concat(chunks).sample(frac=1.).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_df = open_pairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_df.rename(\n",
    "    columns = dict(\n",
    "        zip([\"modern\",\"classical\"],\n",
    "             [\"source\",\"target\"] if TO_CLASSICAL else [\"target\",\"source\",]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>下也。</td>\n",
       "      <td>因为在下面。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>长乐王尉粲甚礼之。</td>\n",
       "      <td>垦銮王幽垩很礼待他。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>太师王舜自莽篡位后，病悸剧，死。</td>\n",
       "      <td>太师王舜自王莽篡夺皇位后，得了心悸病，渐渐加剧，终于病故。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>秋七月丙寅，以旱，亲录京城囚徒。</td>\n",
       "      <td>秋七月二十九日，因为干旱，皇上亲自审查并记录囚徒罪状。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>乙亥，齐仪同三司元旭坐事赐死。</td>\n",
       "      <td>乙亥，北齐国仪同三司元旭因犯罪被赐死。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                         target\n",
       "0               下也。                         因为在下面。\n",
       "1         长乐王尉粲甚礼之。                     垦銮王幽垩很礼待他。\n",
       "2  太师王舜自莽篡位后，病悸剧，死。  太师王舜自王莽篡夺皇位后，得了心悸病，渐渐加剧，终于病故。\n",
       "3  秋七月丙寅，以旱，亲录京城囚徒。    秋七月二十九日，因为干旱，皇上亲自审查并记录囚徒罪状。\n",
       "4   乙亥，齐仪同三司元旭坐事赐死。            乙亥，北齐国仪同三司元旭因犯罪被赐死。"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ukyVGg8HmSd-"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModel,\n",
    "    EncoderDecoderModel\n",
    "    )\n",
    "PRETRAINED = \"bert-base-chinese\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytoch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def combine_randomly(data):\n",
    "    if random.random()>.5:\n",
    "        a,b = data['source'],data['target']\n",
    "    else:\n",
    "        a,b = data['target'],data['source']\n",
    "    return f\"{a}{b}\"\n",
    "\n",
    "def pick_randomly(data):\n",
    "    return list(data.values())[int(random.random()>.5)]\n",
    "\n",
    "def mixup(data):\n",
    "    if len(data['target'])> 70:\n",
    "        th = .7\n",
    "    else:\n",
    "        th = .3\n",
    "    if random.random()>th:\n",
    "        return combine_randomly(data)\n",
    "    else:\n",
    "        return pick_randomly(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLSearch(Dataset):\n",
    "    def __init__(\n",
    "        self, df, tokenizer,\n",
    "        max_len=128,\n",
    "        no_punkt:bool = False,\n",
    "        mlm_probability:float = .15,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        no_punkt, do we ramdomly remove punctuation\n",
    "        from source sentence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mlm_probability = mlm_probability\n",
    "        \n",
    "    def __len__(self, ):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return mixup(dict(self.df.loc[idx]))\n",
    "\n",
    "    def collate(self, data):\n",
    "        inputs = self.tokenizer(\n",
    "            list(data),\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return self.mlm_masking(inputs)\n",
    "    \n",
    "    def mlm_masking(self,inputs):\n",
    "        \"\"\"\n",
    "        convert inputs for masked language modeling\n",
    "        \"\"\"\n",
    "        if self.mlm_probability is None:\n",
    "            return inputs\n",
    "        input_ids = inputs.input_ids\n",
    "        token_type_ids = inputs.token_type_ids\n",
    "        \n",
    "        # masking input_ids\n",
    "        masked = input_ids.clone()\n",
    "        masked[\n",
    "            torch.rand(input_ids.shape).to(input_ids.device) < self.mlm_probability\n",
    "        ] = self.tokenizer.mask_token_id\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        labels[token_type_ids == 1] = -100\n",
    "        labels[labels==0] = -100\n",
    "        token_type_ids[masked==self.tokenizer.mask_token_id] = 1\n",
    "        labels[token_type_ids == 0] = -100\n",
    "        \n",
    "        inputs['input_ids'] = masked\n",
    "        inputs['labels'] = labels\n",
    "        inputs['token_type_ids'] = token_type_ids\n",
    "        return inputs\n",
    "\n",
    "    def dataloader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.collate,\n",
    "        )\n",
    "\n",
    "    def split_train_valid(self, valid_size=0.1):\n",
    "        split_index = int(len(self) * (1 - valid_size))\n",
    "        cls = type(self)\n",
    "        shuffled = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        train_set = cls(\n",
    "            shuffled.iloc[:split_index],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.max_len,\n",
    "        )\n",
    "        valid_set = cls(\n",
    "            shuffled.iloc[split_index:],\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_len=self.max_len,\n",
    "        )\n",
    "        return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = XLSearch(df, tokenizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'又将御史王金，主事马思聪、金山，参议黄宏、许效廉，布政使胡廉，参政陈杲、刘非木，佥事赖凤，指挥许金、白昂等人逮捕下狱。执御史王金，主事马思聪、金山，参议黄宏、许效廉，布政使胡廉，参政陈杲、刘棐，佥事赖凤，指挥许金、白昂等下狱。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways of mixing and masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, df,\n",
    "        tokenizer,\n",
    "        batch_size=12,\n",
    "        max_len=128,\n",
    "        no_punkt:bool=False):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.ds = XLSearch(df,\n",
    "                          tokenizer,\n",
    "                          max_len=max_len,)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set, self.valid_set = self.ds.split_train_valid()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_set.dataloader(\n",
    "            batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.valid_set.dataloader(\n",
    "            batch_size=self.batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    df, tokenizer,\n",
    "    batch_size=64,\n",
    "    max_len=256,\n",
    "    no_punkt=False if TO_CLASSICAL else True,)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1282,  103,  ...,    0,    0,    0],\n",
       "        [ 101, 3293, 1062,  ...,    0,    0,  103],\n",
       "        [ 101,  758, 2399,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 7826,  815,  ...,  103,    0,    0],\n",
       "        [ 101, 5628, 6818,  ...,    0,    0,    0],\n",
       "        [ 101, 5745,  815,  ...,    0,  103,    0]]), 'token_type_ids': tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 1, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100, -100, 1063,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = next(iter(data_module.train_dataloader()))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are doing clasical Chinese to modern Chinese, we can randomly set half of the input without any punctuation, as many data source might be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.batch_decode(\n",
    "#     inputs.input_ids,skip_special_tokens=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92iwRu6Oqbzb"
   },
   "source": [
    "### Load pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pajv5ridLamp"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# loading pretrained model\n",
    "model = AutoModelForMaskedLM.from_pretrained(PRETRAINED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jBVyNeKUv6FU"
   },
   "outputs": [],
   "source": [
    "class MaskedLM(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n",
    "\n",
    "    def accuracy(self, batch_input, outputs):\n",
    "        \"\"\"\n",
    "        Accuracy for masked language model\n",
    "        \"\"\"\n",
    "        mask_mask = batch_input.labels != -100\n",
    "        predictions = outputs.logits.argmax(-1)[mask_mask]\n",
    "        targets = batch_input.labels[mask_mask]\n",
    "        return (predictions == targets).float().mean()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = dict(\n",
    "            input_ids=batch.input_ids,\n",
    "            attention_mask=batch.attention_mask,\n",
    "            labels=batch.labels,\n",
    "            )\n",
    "        outputs = self(**inputs)\n",
    "        self.log(\"loss\", outputs.loss, prog_bar=True)\n",
    "        self.log(\"acc\",\n",
    "            self.accuracy(batch, outputs),\n",
    "            on_step=True, prog_bar=True)\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = dict(\n",
    "            input_ids=batch.input_ids,\n",
    "            attention_mask=batch.attention_mask,\n",
    "            labels=batch.labels,\n",
    "            )\n",
    "        outputs = self(**inputs)\n",
    "        self.log(\"val_loss\", outputs.loss, prog_bar=True)\n",
    "        self.log(\"val_acc\",\n",
    "            self.accuracy(batch, outputs),\n",
    "            on_step=False, prog_bar=True)\n",
    "        return outputs.loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5uIjcPuXw0Fr"
   },
   "outputs": [],
   "source": [
    "module = MaskedLM(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBf3NTKSLcUb"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"xlsearch_cc_zh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | BertForMaskedLM | 102 M \n",
      "------------------------------------------\n",
      "102 M     Trainable params\n",
      "0         Non-trainable params\n",
      "102 M     Total params\n",
      "409.161   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n",
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eaec356ce34562923404d08801bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/properties.py:249: UserWarning: The progress bar already tracks a metric with the name(s) 'loss' and `self.log('loss', ..., prog_bar=True)` will overwrite this value.  If this is undesired, change the name or override `get_progress_bar_dict()` in `LightingModule`.\n",
      "  f\" in `LightingModule`.\", UserWarning\n",
      "Epoch 0, global step 1023: acc reached 0.54819 (best 0.54819), saving model to \"/nvme/GCI/transformers/weights/xlsearch_cc_zh/epoch=0-step=1023.ckpt\" as top 3\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb_logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=f\"/GCI/tensorboard/{TASK}\",\n",
    "    name=TASK,\n",
    "    )\n",
    "\n",
    "save_cb = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f\"/GCI/transformers/weights/{TASK}\",\n",
    "    save_top_k=3,\n",
    "    verbose=True,\n",
    "    monitor=\"acc\",\n",
    "    save_weights_only=True,\n",
    "    every_n_train_steps=1024,\n",
    "    mode=\"max\",\n",
    "    )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[1,],\n",
    "    max_epochs=10,\n",
    "    logger = [tb_logger,],\n",
    "    callbacks=[save_cb,\n",
    "#                DataFrameMetricsCallback()\n",
    "              ],\n",
    "    )\n",
    "\n",
    "trainer.fit(\n",
    "    module,\n",
    "    datamodule = data_module\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = save.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.load_state_dict(torch.load(best, map_location=\"cpu\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20893a4c96924d2ba176c94cf0eb5e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/916M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://user:eOwfuFZJHbcMgbzVtVPDaSGtpbpjumsgTzZtfKlrMbSECzypnCYHZGDhHVsHRsYZzvdrkcxbnnSXRROfqdNRYfMvVfaVSOTxORkEUcMnAPEWXhkWpVEDrgfUZJdmleTx@huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient\n",
      "   08f3b21..5ee2133  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient/commit/5ee213356db17dfa9577226a90d5e9bd9461b495'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder_decoder.push_to_hub(\"raynardj/wenyanwen-chinese-translate-to-ancient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://user:eOwfuFZJHbcMgbzVtVPDaSGtpbpjumsgTzZtfKlrMbSECzypnCYHZGDhHVsHRsYZzvdrkcxbnnSXRROfqdNRYfMvVfaVSOTxORkEUcMnAPEWXhkWpVEDrgfUZJdmleTx@huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient\n",
      "   5ee2133..ab72fa4  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient/commit/ab72fa41627cfeb6fef64e196d68d81b0adb6228'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder_tokenizer.push_to_hub(\"raynardj/wenyanwen-chinese-translate-to-ancient\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "579055f403bf4594a2c665adfdfb8995": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "659eee19636c45da881d243f66aedf27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94111dfb9a2d4a4f93e00bdb34c70090": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff2e02e62d0b438cac9f521da8c0d5eb",
       "IPY_MODEL_fc66ee3afa1944beb42494efbb1301ac",
       "IPY_MODEL_9ac2a1e65c084bca8cdff9f1dc7541e0"
      ],
      "layout": "IPY_MODEL_659eee19636c45da881d243f66aedf27"
     }
    },
    "94765776469249ea94eee8ccf64c47e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ac2a1e65c084bca8cdff9f1dc7541e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94765776469249ea94eee8ccf64c47e7",
      "placeholder": "​",
      "style": "IPY_MODEL_579055f403bf4594a2c665adfdfb8995",
      "value": " 1/1 [00:00&lt;00:00, 21.65it/s]"
     }
    },
    "b3486fd1f15b43068e47df0ad6a81559": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb2e04bed86047b0b3a4e587cfb48ef0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8dad1a95c8646edbde1af6fcc3f0ff9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db73dbc0dabc429481860871b02dc9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc66ee3afa1944beb42494efbb1301ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb2e04bed86047b0b3a4e587cfb48ef0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c8dad1a95c8646edbde1af6fcc3f0ff9",
      "value": 1
     }
    },
    "ff2e02e62d0b438cac9f521da8c0d5eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3486fd1f15b43068e47df0ad6a81559",
      "placeholder": "​",
      "style": "IPY_MODEL_db73dbc0dabc429481860871b02dc9e0",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

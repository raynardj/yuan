{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgebox Imports\n",
    "from forgebox.imports import *\n",
    "from forgebox.category import Category\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer, BertForTokenClassification\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc_utils.env import sys_loc\n",
    "DATA = sys_loc('DATA')/\"nlp\"/\"zh\"/\"daizhigev20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "META = pd.read_csv(DATA/\"meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = META.query(\"charspan<15\").sample(frac=1.).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punkt_regex = r'[^\\w\\s]'\n",
    "\n",
    "def position_of_all_punctuation(x):\n",
    "    return [m.start() for m in re.finditer(punkt_regex, x)]\n",
    "\n",
    "# simplify the punctuation\n",
    "eng_punkt_to_cn_dict = {\n",
    "    \".\": \"。\",\n",
    "    \",\": \"，\",\n",
    "    \":\": \"：\",\n",
    "    \";\": \"；\",\n",
    "    \"?\": \"？\",\n",
    "    \"!\": \"！\",\n",
    "    \"“\": \"\\\"\",\n",
    "    \"”\": \"\\\"\",\n",
    "    \"‘\": \"\\'\",\n",
    "    \"’\": \"\\'\",\n",
    "    \"「\": \"（\",\n",
    "    \"」\": \"）\",\n",
    "    \"『\": \"\\\"\",\n",
    "    \"』\": \"\\\"\",\n",
    "    \"（\": \"（\",\n",
    "    \"）\": \"）\",\n",
    "    \"《\": \"【\",\n",
    "    \"》\": \"】\",\n",
    "    \"［\": \"【\",\n",
    "    \"］\": \"】\",\n",
    "    }\n",
    "\n",
    "def translate_eng_punkt_to_cn(char):\n",
    "    if char == \"O\":\n",
    "        return char\n",
    "    if char in eng_punkt_to_cn_dict.values():\n",
    "        return char\n",
    "    result = eng_punkt_to_cn_dict.get(char)\n",
    "    if result is None:\n",
    "        return \"。\"\n",
    "    return result\n",
    "\n",
    "def punct_ner_pair(sentence):\n",
    "    positions = position_of_all_punctuation(sentence)\n",
    "    x = re.sub(punkt_regex, '', sentence)\n",
    "    y = list(\"O\"*len(x))\n",
    "    \n",
    "    for i, p in enumerate(positions):\n",
    "        y[p-i-1] = sentence[p]\n",
    "    p_df = pd.DataFrame({\"x\":list(x), \"y\":y})\n",
    "    p_df[\"y\"] = p_df[\"y\"].apply(translate_eng_punkt_to_cn)\n",
    "    return p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LABELS = [\"O\",]+list(eng_punkt_to_cn_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cates = Category(ALL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PunctDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        filelist: List[str],\n",
    "        num_threads: int = 8,\n",
    "        length: int = 1000,\n",
    "        size: int = 540\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - filelist: list of file names\n",
    "            - The dataset will open ```num_threads``` files, and hold\n",
    "                in memory simoultaneously.\n",
    "            - num_threads: number of threads to read files,\n",
    "            - length: number of sentences per batch\n",
    "            - size: number of characters per sentence\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.filelist = filelist\n",
    "        self.num_threads = num_threads\n",
    "        self.length = length\n",
    "        # open file strings, index is mod of num_threads\n",
    "        self.current_files = dict(enumerate([\"\"]*length))\n",
    "        self.string_index = dict(enumerate([0]*length))\n",
    "        self.to_open_idx = 0\n",
    "        self.size = size\n",
    "        self.get_counter = 0\n",
    "        self.return_string = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"PunctDataset: {len(self)}, on {len(self.filelist)} files\"\n",
    "\n",
    "    def new_file(self, idx_mod):\n",
    "        filename = self.filelist[self.to_open_idx]\n",
    "        with open(self.data_dir/filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.current_files[idx_mod] = f.read()\n",
    "\n",
    "        self.to_open_idx += 1\n",
    "\n",
    "        # reset to open article file index\n",
    "        if self.to_open_idx >= len(self.filelist):\n",
    "            self.to_open_idx = 0\n",
    "\n",
    "        # reset string_index within new article file\n",
    "        self.string_index[idx_mod] = 0\n",
    "\n",
    "        if self.to_open_idx % 500 == 0:\n",
    "            print(f\"went through files:\\t{self.to_open_idx}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_mod = self.get_counter % self.num_threads\n",
    "\n",
    "        if self.string_index[idx_mod] >= len(self.current_files[idx_mod]):\n",
    "            self.new_file(idx_mod)\n",
    "        string_idx = self.string_index[idx_mod]\n",
    "\n",
    "        # slicing a sentence\n",
    "        sentence = self.current_files[idx_mod][string_idx:string_idx+self.size]\n",
    "\n",
    "        # move the string_index within current article file\n",
    "        self.string_index[idx_mod] += self.size\n",
    "\n",
    "        # move the get_counter\n",
    "        self.get_counter += 1\n",
    "        p_df = punct_ner_pair(sentence)\n",
    "        return list(p_df.x), list(p_df.y)\n",
    "\n",
    "    def align_offsets(\n",
    "        self,\n",
    "        inputs,\n",
    "        text_labels: List[List[str]],\n",
    "        words: List[List[str]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inputs: output if tokenizer\n",
    "        text_labels: labels in form of list of list of strings\n",
    "        words: words in form of list of list of strings\n",
    "        \"\"\"\n",
    "        labels = torch.zeros_like(inputs.input_ids).long()\n",
    "        labels -= 100\n",
    "        text_lables_array = np.empty(labels.shape, dtype=object)\n",
    "        words_array = np.empty(labels.shape, dtype=object)\n",
    "        max_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        for row_id, input_ids in enumerate(inputs.input_ids):\n",
    "            word_pos = inputs.word_ids(row_id)\n",
    "            for idx, pos in enumerate(word_pos):\n",
    "                if pos is None:\n",
    "                    continue\n",
    "                if pos <= max_len:\n",
    "                    labels[row_id, idx] = self.cates.c2i[text_labels[row_id][pos]]\n",
    "                    if self.return_string:\n",
    "                        text_lables_array[row_id,\n",
    "                                          idx] = text_labels[row_id][pos]\n",
    "                        words_array[row_id, idx] = words[row_id][pos]\n",
    "\n",
    "        inputs['labels'] = labels\n",
    "        if self.return_string:\n",
    "            inputs['text_labels'] = text_lables_array.tolist()\n",
    "            inputs['word'] = words_array.tolist()\n",
    "        return inputs\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        \"\"\"\n",
    "        data: list of tuple\n",
    "        \"\"\"\n",
    "        words, text_labels = zip(*data)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            list(words),\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            is_split_into_words=True,\n",
    "            return_offsets_mapping=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        return self.align_offsets(inputs, text_labels, words)\n",
    "\n",
    "    def dataloaders(self, tokenizer, cates, max_len: int = 512, batch_size: int = 32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cates = cates\n",
    "        self.max_len = max_len\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def split(self, ratio: float = 0.9):\n",
    "        \"\"\"\n",
    "        Split the dataset into train and valid\n",
    "        \"\"\"\n",
    "        np.random.shuffle(self.filelist)\n",
    "        split_idx = int(len(self.filelist)*ratio)\n",
    "        train_dataset = PunctDataset(\n",
    "            self.data_dir,\n",
    "            self.filelist[:split_idx],\n",
    "            num_threads=self.num_threads,\n",
    "            length=int(self.length*ratio),\n",
    "            size=self.size,\n",
    "        )\n",
    "        valid_dataset = PunctDataset(\n",
    "            self.data_dir,\n",
    "            self.filelist[split_idx:],\n",
    "            num_threads=self.num_threads,\n",
    "            length=int(self.length*(1-ratio)),\n",
    "            size=self.size,\n",
    "        )\n",
    "        return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset object\n",
    "\n",
    "* Length is the length of the epoch\n",
    "* Size: is the sequence length\n",
    "* num_threads: num of files that is opening at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PunctDataset(DATA, list(LABELS.filepath), num_threads=8, length=10000, size=512)\n",
    "train_ds, valid_ds = ds.split(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightning data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PunctDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_ds, valid_ds, tokenizer, cates, \n",
    "    max_len=512, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.train_ds, self.valid_ds = train_ds, valid_ds\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cates = cates\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def split_data(self):\n",
    "        \n",
    "        return train_ds, valid_ds\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_ds.dataloaders(\n",
    "            self.tokenizer,\n",
    "            self.cates,\n",
    "            self.max_len,\n",
    "            self.batch_size,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.valid_ds.dataloaders(\n",
    "            self.tokenizer,\n",
    "            self.cates,\n",
    "            self.max_len,\n",
    "            self.batch_size*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained model with proper num of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=len(cates),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = PunctDataModule(train_ds, valid_ds, tokenizer, cates,\n",
    "                              batch_size=32,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter(data_module.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 464])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 464])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def view_label(idx=range(0,31)):\n",
    "#     for x,y in zip(inputs['word'][idx], inputs['text_labels'][idx]):\n",
    "#         print(f\"{x}-{y}\", end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER tranining module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forgebox.thunder.callbacks import DataFrameMetricsCallback\n",
    "from forgebox.hf.train import NERModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = NERModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory /GCI/transformers/weights/punkt_ner/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    }
   ],
   "source": [
    "save_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"/GCI/transformers/weights/punkt_ner/\",\n",
    "    save_top_k=2,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    ")\n",
    "df_show = DataFrameMetricsCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the configure_optimizers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "        # discriminative learning rate\n",
    "    param_groups = [\n",
    "            {'params': self.model.bert.parameters(), 'lr': 5e-6},\n",
    "            {'params': self.model.classifier.parameters(), 'lr': 1e-3},\n",
    "        ]\n",
    "    optimizer = torch.optim.Adam(param_groups, lr=1e-3)\n",
    "    return optimizer\n",
    "\n",
    "NERModule.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0],\n",
    "    max_epochs=100,\n",
    "    callbacks=[df_show, save_callback],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = module.load_from_checkpoint(save_callback.best_model_path, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.model.config.id2label = dict(enumerate(cates.i2c))\n",
    "module.model.config.label2id = cates.c2i.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.model = module.model.eval()\n",
    "module.model = module.model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = \"raynardj/classical-chinese-punctuation-guwen-biaodian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70229344f854882bb4e83c42420b9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/388M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://user:eOwfuFZJHbcMgbzVtVPDaSGtpbpjumsgTzZtfKlrMbSECzypnCYHZGDhHVsHRsYZzvdrkcxbnnSXRROfqdNRYfMvVfaVSOTxORkEUcMnAPEWXhkWpVEDrgfUZJdmleTx@huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian\n",
      "   da1b1fa..163772b  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian/commit/163772b14564fa2930b1460f48be30fa7c9f8438'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model.push_to_hub(TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://user:eOwfuFZJHbcMgbzVtVPDaSGtpbpjumsgTzZtfKlrMbSECzypnCYHZGDhHVsHRsYZzvdrkcxbnnSXRROfqdNRYfMvVfaVSOTxORkEUcMnAPEWXhkWpVEDrgfUZJdmleTx@huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian\n",
      "   163772b..c83256b  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian/commit/c83256b9ba08883a91c78512cce496b3cebe27a5'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline(\"ner\",module.model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_sentence(x: str):\n",
    "    outputs = ner(x)\n",
    "    x_list = list(x)\n",
    "    for i, output in enumerate(outputs):\n",
    "        x_list.insert(output['end']+i, output['entity'])\n",
    "    return \"\".join(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'是书虽称文粹，实与地志相表里。东南文献多借。是以有征与范成大呉郡志相辅而行，亦如骖有靳矣。乾隆四十二年三月，恭校上。'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark_sentence(\"\"\"是书虽称文粹实与地志相表里东南文献多借是以有征与范成大呉郡志相辅而行亦如骖有靳矣乾隆四十二年三月恭校上\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'郡邑，置夫子庙于学，以嵗时释奠。盖自唐贞观以来，未之或改。我宋有天下因其制而损益之。姑苏当浙右要区，规模尤大，更建炎戎马，荡然无遗。虽修学宫于荆榛瓦砾之余，独殿宇未遑议也。每春秋展礼于斋庐，已则置不问，殆为阙典。今寳文阁直学士括苍梁公来牧之。明年，实绍兴十有一禩也。二月，上丁修祀既毕，乃愓然自咎，揖诸生而告之曰\"天子不以汝嘉为不肖，俾再守兹土，顾治民事，神皆守之职。惟是夫子之祀，教化所基，尤宜严且谨。而拜跪荐祭之地，卑陋乃尔。其何以掲防妥灵？汝嘉不敢避其责。曩常去此弥年，若有所负，尚安得以罢輭自恕，复累后人乎！他日或克就绪，愿与诸君落之。于是谋之，僚吏搜故府，得遗材千枚，取赢资以给其费。鸠工庀役，各举其任。嵗月讫，工民不与知像，设礼器，百用具修。至于堂室。廊序。门牖。垣墙，皆一新之。'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark_sentence(\"\"\"郡邑置夫子庙于学以嵗时释奠盖自唐贞观以来未之或改我宋有天下因其制而损益之姑苏当浙右要区规模尤大更建炎戎马荡然无遗虽修学宫于荆榛瓦砾之余独殿宇未遑议也每春秋展礼于斋庐已则置不问殆为阙典今寳文阁直学士括苍梁公来牧之明年实绍兴十有一禩也二月上丁修祀既毕乃愓然自咎揖诸生而告之曰天子不以汝嘉为不肖俾再守兹土顾治民事神皆守之职惟是夫子之祀教化所基尤宜严且谨而拜跪荐祭之地卑陋乃尔其何以掲防妥灵汝嘉不敢避其责曩常去此弥年若有所负尚安得以罢輭自恕复累后人乎他日或克就绪愿与诸君落之于是谋之僚吏搜故府得遗材千枚取赢资以给其费鸠工庀役各举其任嵗月讫工民不与知像设礼器百用具修至于堂室廊序门牖垣墙皆一新之\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded [here](https://github.com/chinese-poetry/chinese-poetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgebox Imports\n",
    "from forgebox.imports import *\n",
    "from gc_utils.env import *\n",
    "import pytorch_lightning as pl\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "import random\n",
    "from typing import List\n",
    "import re\n",
    "from jieba import cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_jupyter():\n",
    "    try:\n",
    "        get_ipython()\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "    \n",
    "IS_JUPYTER = is_jupyter()\n",
    "if IS_JUPYTER:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cc_vs_zh', 'cctc', 'cn_shi', 'daizhigev20']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = sys_loc(\"DATA\")/\"nlp\"/\"zh\"\n",
    "DATA.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POET = DATA/\"cn_shi\"\n",
    "ALL_JSON = list(POET.rglob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    return json.loads(Path(path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befb7e3f30e8467793de76ee58339a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2292b347838427aaf06ff41195b30e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ci_dict = dict((str(i),pd.read_json(i))\n",
    "               for i in tqdm(list(\n",
    "                   DATA.rglob(\"cn_shi/ci/ci.song*.json\"))))\n",
    "\n",
    "shi_dict = dict((str(i),pd.read_json(i))\n",
    "               for i in  tqdm(list(\n",
    "                   DATA.rglob(\"cn_shi/json/poet.*.json\"))))\n",
    "\n",
    "all_df = pd.concat(list(ci_dict.values())+list(shi_dict.values()))[\n",
    "    [\"author\",\"paragraphs\",\"rhythmic\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.sample(frac=1.).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = list(all_df[\"paragraphs\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(\n",
    "    paragraphs: List[str], puncts=\"，。？！?,.!\"\n",
    "    ):\n",
    "    text = \"\".join(paragraphs)\n",
    "    num_head = random.choice([2,3,4])\n",
    "    heads = \"\"\n",
    "    return_text = \"\"\n",
    "    last_is_break = True\n",
    "    for i, c in enumerate(text):\n",
    "        if last_is_break:\n",
    "            heads += c\n",
    "            return_text += \"[CLS]\"\n",
    "        else:\n",
    "            return_text += c\n",
    "        if len(heads) >= num_head:\n",
    "            return_text += text[i+1:]\n",
    "            break\n",
    "        if c in puncts:\n",
    "            last_is_break = True\n",
    "        else:\n",
    "            last_is_break = False\n",
    "    return heads, return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('間曜翩過',\n",
       " '[CLS]維有常度，[CLS]靈無停輈。[CLS]翩葉辭柯，[CLS]眼綠已稠。弱榦不盈尺，忽已高岑樓。念昔過庭日，朋來悉良儔。我年未成童，子少無與侔。我質本駑駘，蹇步畏阻脩。子如渥洼駒，猛氣已食牛。當時二老人，笑語懽且酬。門戶各有托，寧計才與不。登門如昨日，星紀跡再周。二老安在哉，體魄歸山丘。隔屋聞讀書，玉樹鏘琳球。呼燈使來前，秀氣炯雙眸。問之垂九齡，屬對解冥搜。感此傷我心，淚下不可收。來者日已長，逝者挽不留。其間我與子，能閲幾春秋。寧復青衿佩，與子從親游。幸子齒猶壯，有母方白頭。刷翮凌青霄，足勝負米由。而我風樹悲，耿耿何時休。四十已無聞，過是夫何求。矧復病日侵，見面良可羞。竹實不療饑，芰製非寒裘。躬耕苦勤勞，代耕多悔尤。學仙竟誰成，百年等浮漚。俛仰天地間，身世真悠悠。時雨漲綠池，好風交平疇。嚶嚶出谷鳥，汎汎川上鷗。遇景適會心，曠望聊夷猶。')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "def cutting(text):\n",
    "    return list(i for i in cut(replace_punctuation(text), HMM=True,)if i != ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.665 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['春眠', '不觉', '晓', '处处', '闻啼鸟']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutting(\"春眠不觉晓， 处处闻啼鸟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_and_shuffle(li, min_n:int=0, max_n:int=None):\n",
    "    if max_n is None:\n",
    "        max_n = int(len(li)*.7)\n",
    "    n = min_n + random.randint(0, min(max_n - min_n,10))\n",
    "    random.shuffle(li)\n",
    "    return list(set(li[:n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kw(text):\n",
    "    return pick_and_shuffle(cutting(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['晓', '春眠', '不觉']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_kw(\"春眠不觉晓， 处处闻啼鸟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads, headless = extract(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('間曜翩過', ['星紀跡', '我', '何求', '駘', '身世', '風樹悲', '雙眸', '復', '托', '與子'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads, create_kw(headless.replace('[CLS]',\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        tokenizer,\n",
    "        p_head:float=.2,\n",
    "    ):\n",
    "        self.df = df.sample(frac=1).reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p_head = p_head\n",
    "        self.cn_num_dict = dict((i+1,f\"『{c}』\") for i, c in enumerate(\"一二三四\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        paragraphs = row.paragraphs\n",
    "        heads, headless = extract(paragraphs)\n",
    "        kws = '-'.join(create_kw(headless.replace('[CLS]',\"\")))\n",
    "        return f\"{kws}《{heads}》{self.cn_num_dict.get(len(heads))}{headless}\"\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        texts = list(batch)\n",
    "        batch = self.tokenizer(\n",
    "            list(texts),\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "        labels = batch['input_ids'].clone()\n",
    "        labels[labels==0] = -100\n",
    "        batch['labels'] = labels\n",
    "        return batch\n",
    "    \n",
    "    def dataloader(self, batch_size=32, shuffle=True):\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def split(self, val_ratio=.05):\n",
    "        df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        train_df = df[:int(len(df)*(1-val_ratio))]\n",
    "        val_df = df[int(len(df)*(1-val_ratio)):]\n",
    "        return PoetDataset(train_df, tokenizer=self.tokenizer),\\\n",
    "            PoetDataset(val_df, tokenizer=self.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "poet_ds = PoetDataset(all_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's arrange the text data this way, so the casual language modeling will work it's own magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'忍看-窈窕-孤寝-勾带-嫩-黄昏《粉度》『二』[CLS]堞云齐，[CLS]清笳、愁入暮烟林杪。素艳透春，玉骨凄凉，勾带月痕生早。江天苍莽黄昏後，依然是、粉寒香瘦。动追感、西园嫩约，夜深人悄。记得东风窈窕。曾夜踏横斜，醉携娇小。惆怅旧欢，回首俱非，忍看绿笺红豆。香销纸帐人孤寝，相思恨、花还知否。梦回处，霜飞翠楼已晓。'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poet_ds[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = poet_ds.dataloader(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-poem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset, self.val_dataset = self.dataset.split()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dataset.dataloader(\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_dataset.dataloader(\n",
    "            batch_size = self.batch_size*2,\n",
    "            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMModule(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch.labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch.labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(poet_ds, batch_size=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = CausalLMModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory /GCI/transformers/weights/kw_leading_po exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "save = pl.callbacks.ModelCheckpoint(\n",
    "    '/GCI/transformers/weights/kw_leading_po',\n",
    "    save_top_k=2,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[1],\n",
    "    max_epochs=6,\n",
    "    callbacks=[save],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 103 M \n",
      "------------------------------------------\n",
      "103 M     Trainable params\n",
      "0         Non-trainable params\n",
      "103 M     Total params\n",
      "412.665   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n",
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bb9a74f1ad407ca77e56df07f35b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:897: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(module, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.load_state_dict(\n",
    "        torch.load(str(save.best), map_location=\"cpu\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = module.model\n",
    "model = model.cpu()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(hub/\"kw-lead-po\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"raynardj/keywords-cangtou-chinese-poetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(lead):\n",
    "    leading = f\"《{lead}》\"\n",
    "    input_ids = tokenizer(leading, return_tensors='pt', ).input_ids\n",
    "    with torch.no_grad():\n",
    "        pred = model.generate(\n",
    "            input_ids,\n",
    "            max_length=256,\n",
    "            num_beams=3,\n",
    "#             do_sample=True,\n",
    "#             top_p=.6,\n",
    "            bos_token_id=tokenizer.sep_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.sep_token_id,\n",
    "        )\n",
    "    print(pred)\n",
    "    return tokenizer.batch_decode(pred, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
